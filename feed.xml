<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2024-05-26T03:32:56+00:00</updated><id>/feed.xml</id><title type="html">PALM Lab</title><subtitle>The Perception, Attention and Memory Lab at the University of Adelaide.</subtitle><entry><title type="html">Associative learning changes multivariate neural signatures of visual working memory</title><link href="/2024/05/21/vss2024-5.html" rel="alternate" type="text/html" title="Associative learning changes multivariate neural signatures of visual working memory" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-26T03:31:02+00:00</updated><id>/2024/05/21/vss2024-5</id><content type="html" xml:base="/2024/05/21/vss2024-5.html"><![CDATA[<p>Talk presentation by William Ngiam at VSS 2024 on how training to learn color pairs and color triplets changes multivariate neural signatures of visual working memory load.</p>

<h3 id="slides">Slides</h3>

<object data="https://palm-lab.github.io/files/Ngiam_VSS2024.pdf" type="application/pdf" width="100%" height="800px">
    <embed src="https://palm-lab.github.io/files/Ngiam_VSS2024.pdf" />
        <p>This browser does not support PDFs. Please download the PDF to view it: <a href="https://palm-lab.github.io/files/Ngiam_VSS2024.pdf">View as PDF</a>.</p>
    &lt;/embed&gt;
</object>
<p><u><a href="https://palm-lab.github.io/files/Ngiam_VSS2024.pdf">Download PDF</a></u><br /></p>

<h3 id="abstract">Abstract</h3>

<p>A hallmark of visual working memory is its sharp capacity limit, though this limit can be circumvented using learned knowledge. For example, when arrays of to-be-remembered items contain statistical regularities, people can learn the associations between items and recall more information overall (Brady et al., 2009; Ngiam et al., 2019). One proposed mechanism for how this recall benefit is achieved is through ‘memory compression’ – redundancies introduce a reduction of information per item, enabling more items to be stored online. Another proposed mechanism is that pointers are efficiently allocated to each ‘chunk’ with the benefit coming from long-term memory retrieval rather than changes to working memory itself. In an attempt to distinguish between these possibilities, we turned to an EEG measure that tracks the number of individuated items stored in working memory (mvLoad; Thyer et al., 2022). The memory compression account predicts an overall increase in the number of items stored online, whereas the long-term memory retrieval account predicts a reduction in working memory load. Subjects completed a training session where they learned specific color-color pairs. In a subsequent EEG session, subjects completed a recall task with 2 random colors, 4 random colors, or 2 learned color pairs. mvLoad analysis showed a reduction in working memory load for the 2 learned pairs condition (from 4 towards 2), consistent with the notion that an item-based pointer is assigned to each chunk. Moreover, multidimensional scaling shows an additional independent signal that distinguishes the 2 learned pairs condition from the other conditions. We propose that this additional signal reflects the involvement of long-term memory, consistent with the notion that the learned association is being relied upon to maintain the information.</p>]]></content><author><name>william-ngiam</name></author><category term="VSS" /><category term="talk" /><category term="long-term memory" /><category term="learning" /><category term="chunking" /><category term="mvLoad" /><summary type="html"><![CDATA[Talk presentation by William Ngiam at VSS 2024 on how training to learn color pairs and color triplets changes multivariate neural signatures of visual working memory load.]]></summary></entry><entry><title type="html">Associative learning changes multivariate neural signatures of working memory load</title><link href="/2023/11/16/opam2023-4.html" rel="alternate" type="text/html" title="Associative learning changes multivariate neural signatures of working memory load" /><published>2023-11-16T00:00:00+00:00</published><updated>2024-05-26T03:31:02+00:00</updated><id>/2023/11/16/opam2023-4</id><content type="html" xml:base="/2023/11/16/opam2023-4.html"><![CDATA[<p>A conference poster presented by William Ngiam at OPAM 2023 showing training to learn color pairs changes neural signatures of working memory load.</p>

<object data="https://palm-lab.github.io/images/posters/OPAM2023.pdf" type="application/pdf" width="100%" height="800px">
    <embed src="https://palm-lab.github.io/images/posters/OPAM2023.pdf" />
        <p>This browser does not support PDFs. Please download the PDF to view it: <a href="https://palm-lab.github.io/images/posters/OPAM2023.pdf">View as PDF</a>.</p>
    &lt;/embed&gt;
</object>
<p><u><a href="https://palm-lab.github.io/images/posters/OPAM2023.pdf">Download PDF</a></u><br /></p>]]></content><author><name>william-ngiam</name></author><category term="opam" /><category term="poster" /><category term="long-term memory" /><category term="mvLoad" /><summary type="html"><![CDATA[A conference poster presented by William Ngiam at OPAM 2023 showing training to learn color pairs changes neural signatures of working memory load.]]></summary></entry><entry><title type="html">Probing working memory pointers by examining contralateral delay activity with moving and updating stimuli</title><link href="/2023/05/19/vss-3.html" rel="alternate" type="text/html" title="Probing working memory pointers by examining contralateral delay activity with moving and updating stimuli" /><published>2023-05-19T00:00:00+00:00</published><updated>2024-05-26T03:31:02+00:00</updated><id>/2023/05/19/vss-3</id><content type="html" xml:base="/2023/05/19/vss-3.html"><![CDATA[<p>A pre-data poster presented by Piotr Styrkowiec and William Ngiam at the Vision Sciences Society Meeting in 2023.</p>

<object data="https://palm-lab.github.io/images/posters/VSS2023.pdf" type="application/pdf" width="100%" height="800px">
    <embed src="https://palm-lab.github.io/images/postersVSS2023.pdf" />
        <p>This browser does not support PDFs. Please download the PDF to view it: <a href="https://palm-lab.github.io/images/posters/VSS2023.pdf">View as PDF</a>.</p>
    &lt;/embed&gt;
</object>
<p><u><a href="https://palm-lab.github.io/images/posters/VSS2023.pdf">Download PDF</a></u><br /></p>]]></content><author><name>piotr-styrkowiec</name></author><category term="vss" /><category term="contralateral delay activity" /><category term="multiple-object tracking" /><summary type="html"><![CDATA[A pre-data poster presented by Piotr Styrkowiec and William Ngiam at the Vision Sciences Society Meeting in 2023.]]></summary></entry><entry><title type="html">Evidence for object-based encoding into visual working memory</title><link href="/2022/05/13/vss2022-2.html" rel="alternate" type="text/html" title="Evidence for object-based encoding into visual working memory" /><published>2022-05-13T00:00:00+00:00</published><updated>2024-05-26T03:31:02+00:00</updated><id>/2022/05/13/vss2022-2</id><content type="html" xml:base="/2022/05/13/vss2022-2.html"><![CDATA[<p>A conference poster presented by William Ngiam at the Vision Sciences Society Meeting in 2022, showing evidence for object-based encoding limits in visual working memory using a novel conjunction whole-report paradigm.</p>

<object data="https://palm-lab.github.io/images/posters/VSS2022.pdf" type="application/pdf" width="100%" height="800px">
    <embed src="https://palm-lab.github.io/images/posters/VSS2022.pdf" />
        <p>This browser does not support PDFs. Please download the PDF to view it: <a href="https://palm-lab.github.io/images/posters/VSS2022.pdf">View as PDF</a>.</p>
    &lt;/embed&gt;
</object>
<p><u><a href="https://palm-lab.github.io/images/posters/VSS2022.pdf">Download PDF</a></u><br /></p>]]></content><author><name>william-ngiam</name></author><category term="vss," /><category term="poster," /><category term="whole-report," /><category term="capacity" /><category term="limits," /><category term="pointers" /><summary type="html"><![CDATA[A conference poster presented by William Ngiam at the Vision Sciences Society Meeting in 2022, showing evidence for object-based encoding limits in visual working memory using a novel conjunction whole-report paradigm.]]></summary></entry><entry><title type="html">Memory compression effects in visual working memory are contingent on explicit long-term memory</title><link href="/2019/11/15/pnom-1.html" rel="alternate" type="text/html" title="Memory compression effects in visual working memory are contingent on explicit long-term memory" /><published>2019-11-15T00:00:00+00:00</published><updated>2024-05-26T03:31:02+00:00</updated><id>/2019/11/15/pnom-1</id><content type="html" xml:base="/2019/11/15/pnom-1.html"><![CDATA[<p>A conference poster presented by William Ngiam at the Psychonomics conference in 2019, showing benefits of statistical learning required explicit awareness of the statistical regularities, suggesting a long-term memory account for the benefit.</p>

<object data="https://palm-lab.github.io/images/posters/psychonomics2019.pdf" type="application/pdf" width="100%" height="800px">
    <embed src="https://palm-lab.github.io/images/posters/psychonomics2019.pdf" />
        <p>This browser does not support PDFs. Please download the PDF to view it: <a href="https://palm-lab.github.io/images/posters/psychonomics2019.pdf">View as PDF</a>.</p>
    &lt;/embed&gt;
</object>
<p><u><a href="https://palm-lab.github.io/images/posters/psychonomics2019.pdf">Download PDF</a></u></p>]]></content><author><name>william-ngiam</name></author><category term="long-term" /><category term="memory," /><category term="chunking," /><category term="psychonomics," /><category term="poster" /><summary type="html"><![CDATA[A conference poster presented by William Ngiam at the Psychonomics conference in 2019, showing benefits of statistical learning required explicit awareness of the statistical regularities, suggesting a long-term memory account for the benefit.]]></summary></entry></feed>